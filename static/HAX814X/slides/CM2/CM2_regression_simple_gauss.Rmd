---
title: 'RÃ©gression simple - II'
author: "Paul Bastide - Benjamin Charlier"
date: "19/01/2022"
output:
  ioslides_presentation:
    fig_width: 7
    fig_height: 4
  self_contained: true
---
  
<!-- ************************************************************************ -->
# What we know
<!-- ************************************************************************ -->
  
## Model

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \quad \forall 1 \leq i \leq n$$

* $y_i$: quantitative response for $i$ (`sales`)
* $x_i$: quantitative predicting variable for $i$ (`TV`)
* $\epsilon_i$: "error" for $i$
  * **random variable**
  * (H1) $\mathbb{E}[\epsilon_i] = 0$ for all $i$ [(centered)]{style="float:right"}
  * (H2) $\mathbb{Var}[\epsilon_i] = \sigma^2$ for all $i$ [(identical variance)]{style="float:right"}
  * (H3) $\mathbb{Cov}[\epsilon_i; \epsilon_j] = 0$ for all $i \neq j$ [(independent)]{style="float:right"}

>* **Note**: Assumptions on first and second moments *only*.  

## OLS Estimators

$$ (\hat{\beta}_0, \hat{\beta}_1) 
= \underset{(\beta_0, \beta_1) \in \mathbb{R}^2}{\operatorname{argmin}} 
\left\{
\sum_{i = 1}^n (y_i - \beta_0 - \beta_1 x_i)^2
\right\}$$

* Properties on the first and second moments *only*.


$$
\begin{aligned}
\hat{\beta}_0
&= \bar{y} - \hat{\beta}_1\bar{x}
&
\hat{\beta}_1
&= \frac{s_{\mathbf{y},\mathbf{x}}^2}{s_{\mathbf{x}}^2}
\\
\mathbb{E}[\hat{\beta}_0] &= \beta_0
&
\mathbb{E}[\hat{\beta}_1] &= \beta_1
\\
\mathbb{Var}[\hat{\beta}_0]
&= \frac{\sigma^2}{n} \left( 1 + \frac{\bar{x}^2}{s_{\mathbf{x}}^2} \right)
&
\mathbb{Var}[\hat{\beta}_1]
&= \frac{\sigma^2}{n}\frac{1}{s_{\mathbf{x}}^2}
\\
\mathbb{Cov}[\hat{\beta}_0; \hat{\beta}_1]
&= - \frac{\sigma^2}{n} \frac{\bar{x}}{s_{\mathbf{x}}^2}
\end{aligned}
$$

## Questions

* Confidence interval for $(\hat{\beta}_0, \hat{\beta}_1)$ ?

* Can we test $\hat{\beta}_1 = 0$ (i.e. no linear trend) ?

>* Assumptions on the moments are not enough.  
$\hookrightarrow$ We need assumptions on the *specific distribution* of the $\epsilon_i$.

>* Most common assumption: $\epsilon_i$ are **Gaussian**.

<!-- ************************************************************************ -->
# Gaussian Model
<!-- ************************************************************************ -->

## Gaussian Model

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \quad \forall 1 \leq i \leq n$$

* $y_i$: quantitative response for $i$ (`sales`)
* $x_i$: quantitative predicting variable for $i$ (`TV`)
* $\epsilon_i$: "error" for $i$
  * random variable (r.v.)
  * independent, **identically distributed** (**iid**), **Gaussian**
  * Centered, with variance $\sigma^2$:

$$
\epsilon_i 
\underset{\text{iid}}{\operatorname{\sim}}
\mathcal{N}(0, \sigma^2) 
\quad 
\text{for}
\quad
1 \leq i \leq n
$$

* [**Note** (H1), (H2) and (H3) are still verified.]

## Reminder: Gaussian Distribution 1/2

Let $X$ be a Gaussian r.v. with expectation $\mu$ and variance $\sigma^2$:
$$
X \sim \mathcal{N}(\mu, \sigma^2).
$$

It admits a probability density function (pdf):
$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
\quad
\forall x \in \mathbb{R},
$$

And a characteristic function:
$$
\phi_{X}(t) = \mathbb{E}[e^{itX}] = e^{i\mu t - \frac{\sigma^2 t^2}{2}}
\quad
\forall t \in \mathbb{R}.
$$

## Reminder: Gaussian Distribution 2/2

```{r gaussian, echo=FALSE, fig.height=5.5, fig.width=8}
x <- -1500:1500 / 100
ccc <- hcl.colors(4)
# Normal
mu <- 0; sigma2 <- 1
plot(x, dnorm(x, mu, sqrt(sigma2)), type = "l", ylab = "f(x)", col = ccc[1], lwd = 2)
segments(x0 = 0, y0 = 0, x1 = 0, y1 = dnorm(mu, mu, sqrt(sigma2)), lty = 2, col = ccc[1])
# mean
mu <- 10; sigma2 <- 1
lines(x, dnorm(x, mu, sqrt(sigma2)), col = ccc[2], lwd = 2)
segments(x0 = mu, y0 = 0, x1 = mu, y1 = dnorm(mu, mu, sqrt(sigma2)), lty = 2, col = ccc[2])
# variance
mu <- 0; sigma2 <- 25
lines(x, dnorm(x, mu, sqrt(sigma2)), col = ccc[3], lwd = 2)
segments(x0 = mu, y0 = 0, x1 = mu, y1 = dnorm(mu, mu, sqrt(sigma2)), lty = 2, col = ccc[3])
# legend
legend("topleft",
       c(expression(paste(mu == 0, "   ; ", sigma^2 == 1)),
         expression(paste(mu == 10, " ; ", sigma^2 == 1)),
         expression(paste(mu == 0, "   ; ", sigma^2 == 25))),
       col = ccc,
       lwd = 2, lty = 1)
```

## Distribution of $\mathbf{y}$

* Model:
$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\quad 
\text{with}
\quad
\epsilon_i 
\underset{\text{iid}}{\operatorname{\sim}}
\mathcal{N}(0, \sigma^2) 
\quad 
\text{for}
\quad
1 \leq i \leq n
$$

>* Distribution of $y_i$ ?

## Reminder: <br> Linear function of a Gaussian

If $X \sim \mathcal{N}(\mu, \sigma^2)$, then for any $(a, b) \in \mathbb{R}^2$,  
$$
Y = aX + b \sim \mathcal{N}(a\mu + b, a^2\sigma^2).
$$

>* **Proof**: Exercise.  
[Hint: use the characteristic function.]

>* $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$  
Apply the property with $a = 1$ and $b = \beta_0 + \beta_1 x_i$.  
[Reminder: $x_i$ are assumed to be non random here.]

## Distribution of $\mathbf{y}$

* Model:
$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\quad 
\text{with}
\quad
\epsilon_i 
\underset{\text{iid}}{\operatorname{\sim}}
\mathcal{N}(0, \sigma^2) 
\quad 
\text{for}
\quad
1 \leq i \leq n
$$

* Distribution of $y_i$:
$$
y_i 
\underset{\text{iid}}{\operatorname{\sim}}
\mathcal{N}(\beta_0 + \beta_1 x_i, \sigma^2) 
\quad 
\text{for}
\quad
1 \leq i \leq n
$$

>* **Question**: What is the *likelihood* of $\mathbf{y} = (y_1, \dotsc, y_n)^T$ ?

<!-- ************************************************************************ -->
# Maximum Likelihood Estimators
<!-- ************************************************************************ -->

## Reminder: Likelihood

If $X$ is a rv that admits a density $f_{\theta}$ that depends on a parameter $\theta$,
then the likelihood function is the density,
seen as a function of $\theta$ given an outcome $X = x$:
$$
\theta \mapsto L(\theta | x) = f_{\theta}(x).
$$

**Example**:  
Assume that we observe one realization $x$ of a Gaussian random variable $X$,
with unknown expectation $\mu$, but known variance $\sigma^2 = 1$.  
Then the Likelihood of the observation $x_{obs}$ is the function:
$$
\mu \to \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x_{obs} - \mu)^2}{2\sigma^2}\right).
$$

## Reminder: Maximum Likelihood 1/2

The Maximum Likelihood (ML) estimator $\hat{\theta}$ of the parameter $\theta$ is the
one that maximizes the likelihood function in $\theta$, given an observation $x$:

$$
\hat{\theta} = \underset{\theta}{\operatorname{argmax}} L(\theta | x).
$$
**Example**:  
Let $x_{obs}$ be one realization of a Gaussian r.v. $X$,
with unknown expectation $\mu$, but known variance $\sigma^2 = 1$.  
Then the ML estimator $\hat{\mu}$ of $\mu$ is:
$$
\hat{\mu} = \underset{\mu \in \mathbb{R}}{\operatorname{argmax}} \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x_{obs} - \mu)^2}{2\sigma^2}\right).
$$

## Reminder: Maximum Likelihood 2/2

```{r ml, echo=FALSE, fig.height=5.5, fig.width=8}
x <- -500:500 / 100
xobs <- 1
par(mar = c(5, 8, 4, 0) + 0.1)
# ML
mu <- xobs; sigma2 <- 1
plot(x, dnorm(x, mu, sqrt(sigma2)), type = "l", ylab = "f(x)", col = "firebrick3", lwd = 0)
segments(x0 = mu, y0 = 0, x1 = mu, y1 = dnorm(mu, mu, sqrt(sigma2)), lty = 2, col = "firebrick3", lwd = 2)
axis(side = 1, at = xobs, labels = expression(x[obs]), col = "firebrick3",  col.axis = "firebrick3", lwd = 2)
# mean
mu <- -1; sigma2 <- 1
lines(x, dnorm(x, mu, sqrt(sigma2)), lwd = 2)
segments(x0 = xobs, y0 = dnorm(xobs, mu, sqrt(sigma2)), x1 = -10, y1 = dnorm(xobs, mu, sqrt(sigma2)), lty = 2, lwd = 2)
axis(side = 2, at = dnorm(xobs, mu, sqrt(sigma2)), labels = expression(paste(L(mu == -1*"|"*x[obs]))), las = 1, hadj = 1.2)
```

## Reminder: Maximum Likelihood 2/2

```{r ml2, echo=FALSE, fig.height=5.5, fig.width=8}
x <- -500:500 / 100
xobs <- 1
par(mar = c(5, 8, 4, 0) + 0.1)
# ML
mu <- xobs; sigma2 <- 1
plot(x, dnorm(x, mu, sqrt(sigma2)), type = "l", ylab = "f(x)", col = "firebrick3", lwd = 0)
segments(x0 = mu, y0 = 0, x1 = mu, y1 = dnorm(mu, mu, sqrt(sigma2)), lty = 2, col = "firebrick3", lwd = 2)
axis(side = 1, at = xobs, labels = expression(x[obs]), col = "firebrick3",  col.axis = "firebrick3", lwd = 2)
# mean
mu <- 0; sigma2 <- 1
lines(x, dnorm(x, mu, sqrt(sigma2)), lwd = 2)
segments(x0 = xobs, y0 = dnorm(xobs, mu, sqrt(sigma2)), x1 = -10, y1 = dnorm(xobs, mu, sqrt(sigma2)), lty = 2, lwd = 2)
axis(side = 2, at = dnorm(xobs, mu, sqrt(sigma2)), labels = expression(paste(L(mu == 0*"|"*x[obs]))), las = 1, hadj = 1.2, lwd = 2)
# mean
mu <- -1; sigma2 <- 1
lines(x, dnorm(x, mu, sqrt(sigma2)), lwd = 0.5)
segments(x0 = xobs, y0 = dnorm(xobs, mu, sqrt(sigma2)), x1 = -10, y1 = dnorm(xobs, mu, sqrt(sigma2)), lty = 2, lwd = 0.5)
axis(side = 2, at = dnorm(xobs, mu, sqrt(sigma2)), labels = expression(paste(L(mu == -1*"|"*x[obs]))), las = 1, hadj = 1.2, lwd = 0.5)
```

## Reminder: Maximum Likelihood 2/2

```{r ml3, echo=FALSE, fig.height=5.5, fig.width=8}
x <- -500:500 / 100
xobs <- 1
par(mar = c(5, 8, 4, 0) + 0.1)
# ML
mu <- xobs; sigma2 <- 1
plot(x, dnorm(x, mu, sqrt(sigma2)), type = "l", ylab = "f(x)", col = "firebrick3", lwd = 2)
segments(x0 = mu, y0 = 0, x1 = mu, y1 = dnorm(mu, mu, sqrt(sigma2)), lty = 2, col = "firebrick3", lwd = 2)
axis(side = 1, at = xobs, labels = expression(x[obs]), col = "firebrick3",  col.axis = "firebrick3", lwd = 2)
segments(x0 = xobs, y0 = dnorm(xobs, mu, sqrt(sigma2)), x1 = -10, y1 = dnorm(xobs, mu, sqrt(sigma2)), lty = 2, col = "firebrick3", lwd = 2)
axis(side = 2, at = dnorm(xobs, mu, sqrt(sigma2)), labels = expression(paste(L(mu == x[obs]*"|"*x[obs]))), las = 1, col = "firebrick3",  col.axis = "firebrick3", hadj = 1.2)
# mean
mu <- 0; sigma2 <- 1
lines(x, dnorm(x, mu, sqrt(sigma2)), lwd = 0.5)
segments(x0 = xobs, y0 = dnorm(xobs, mu, sqrt(sigma2)), x1 = -10, y1 = dnorm(xobs, mu, sqrt(sigma2)), lty = 2, lwd = 0.5)
axis(side = 2, at = dnorm(xobs, mu, sqrt(sigma2)), labels = expression(paste(L(mu == 0*"|"*x[obs]))), las = 1, hadj = 1.2, lwd = 0.5)
# mean
mu <- -1; sigma2 <- 1
lines(x, dnorm(x, mu, sqrt(sigma2)), lwd = 0.5)
segments(x0 = xobs, y0 = dnorm(xobs, mu, sqrt(sigma2)), x1 = -10, y1 = dnorm(xobs, mu, sqrt(sigma2)), lty = 2, lwd = 0.5)
axis(side = 2, at = dnorm(xobs, mu, sqrt(sigma2)), labels = expression(paste(L(mu == -1*"|"*x[obs]))), las = 1, hadj = 1.2, lwd = 0.5)
```

## Likelihood of $\mathbf{y}$

* Distribution of $y_i$:
$$
y_i 
\underset{\text{iid}}{\operatorname{\sim}}
\mathcal{N}(\beta_0 + \beta_1 x_i, \sigma^2) 
\quad 
\text{for}
\quad
1 \leq i \leq n
$$

* Likelihood of $\mathbf{y} = (y_1, \dotsc, y_n)^T$:
$$
\begin{aligned}
&L(\beta_0, \beta_1, \sigma^2 | y_1, \dotsc, y_n)
= \prod_{i = 1}^n L(\beta_0, \beta_1, \sigma^2 | y_i) & [ind.]\\
\end{aligned}
$$

and

$$
\begin{aligned}
L(\beta_0, \beta_1, \sigma^2 | y_i)
= \cdots
\end{aligned}
$$

## Likelihood of $\mathbf{y}$

* Distribution of $y_i$:
$$
y_i 
\underset{\text{iid}}{\operatorname{\sim}}
\mathcal{N}(\beta_0 + \beta_1 x_i, \sigma^2) 
\quad 
\text{for}
\quad
1 \leq i \leq n
$$

* Likelihood of $\mathbf{y} = (y_1, \dotsc, y_n)^T$:
$$
\begin{aligned}
&L(\beta_0, \beta_1, \sigma^2 | y_1, \dotsc, y_n)
= \prod_{i = 1}^n L(\beta_0, \beta_1, \sigma^2 | y_i) & [ind.]\\
\end{aligned}
$$

and

$$
\begin{aligned}
L(\beta_0, \beta_1, \sigma^2 | y_i)
= \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2\sigma^2}\right)
\end{aligned}
$$

## Likelihood of $\mathbf{y}$

* Likelihood of $\mathbf{y} = (y_1, \dotsc, y_n)^T$:
$$
\begin{aligned}
&L(\beta_0, \beta_1, \sigma^2 | y_1, \dotsc, y_n)
= \prod_{i = 1}^n L(\beta_0, \beta_1, \sigma^2 | y_i)\\
&\qquad = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2\sigma^2}\right)\\
&\qquad = \frac{1}{\left(\sqrt{2\pi\sigma^2}\right)^n}\exp\left(-\frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i - \beta_0 - \beta_1 x_i)^2\right)\\
&\log L(\beta_0, \beta_1, \sigma^2 | y_1, \dotsc, y_n) = \cdots
\end{aligned}
$$

## Log-Likelihood of $\mathbf{y}$

$$
\begin{aligned}
&\log L(\beta_0, \beta_1, \sigma^2 | y_1, \dotsc, y_n) = \\
&\qquad  - \frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i - \beta_0 - \beta_1 x_i)^2\\
\end{aligned}
$$

>* **Question**: What are the Maximum Likelihood estimators of $\beta_0$, $\beta_1$ and $\sigma^2$ ?

>* We need to find:
$$
(\hat{\beta}_0^{ML}, \hat{\beta}_1^{ML}, \hat{\sigma}^2_{ML}) 
= \underset{(\beta_0, \beta_1, \sigma^2) \in \mathbb{R}^2\times\mathbb{R}_+^*}{\operatorname{argmax}} 
\log L(\beta_0, \beta_1, \sigma^2 | \mathbf{y})
$$

## ML Estimators - $\beta_0$ and $\beta_1$

$$
\begin{aligned}
&\log L(\beta_0, \beta_1, \sigma^2 | \mathbf{y}) = \\
&\qquad  - \frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i - \beta_0 - \beta_1 x_i)^2\\
\end{aligned}
$$

## ML Estimators - $\beta_0$ and $\beta_1$

$$
\begin{aligned}
&\log L(\beta_0, \beta_1, \sigma^2 | \mathbf{y}) = \\
&\qquad  - \frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\underbrace{\sum_{i = 1}^n (y_i - \beta_0 - \beta_1 x_i)^2}_{\text{Sum of Squares } SS(\beta_0, \beta_1)}\\
\end{aligned}
$$

>* For any $\sigma^2 > 0$,
$$
\begin{aligned}
(\hat{\beta}_0^{ML}, \hat{\beta}_1^{ML}) 
&= \underset{(\beta_0, \beta_1) \in \mathbb{R}^2}{\operatorname{argmax}} 
\log L(\beta_0, \beta_1, \sigma^2 | \mathbf{y})\\
&=  \underset{(\beta_0, \beta_1) \in \mathbb{R}^2}{\operatorname{argmin}} SS(\beta_0, \beta_1)\\
&= (\hat{\beta}_0^{OLS}, \hat{\beta}_1^{OLS}) 
\end{aligned}
$$

## ML Estimators - $\beta_0$ and $\beta_1$

$$
\begin{aligned}
&\log L(\beta_0, \beta_1, \sigma^2 | \mathbf{y}) = \\
&\qquad  - \frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\underbrace{\sum_{i = 1}^n (y_i - \beta_0 - \beta_1 x_i)^2}_{\text{Sum of Squares } SS(\beta_0, \beta_1)}\\
\end{aligned}
$$

$$
(\hat{\beta}_0^{ML}, \hat{\beta}_1^{ML}) = (\hat{\beta}_0^{OLS}, \hat{\beta}_1^{OLS}) 
$$

$\hookrightarrow$ The ML estimators are the same as the OLS estimators.

## ML Estimators - $\sigma^2$

$$
\begin{aligned}
\hat{\sigma}^2_{ML}
&= \underset{\sigma^2 \in \mathbb{R}_+^*}{\operatorname{argmax}} 
\log L(\hat{\beta}_0, \hat{\beta}_1, \sigma^2 | \mathbf{y})\\
&=  \underset{\sigma^2 \in \mathbb{R}_+^*}{\operatorname{argmax}} 
- \frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2\\
&=  \underset{\sigma^2 \in \mathbb{R}_+^*}{\operatorname{argmin}} 
\frac{n}{2} \log(2\pi\sigma^2) + \frac{1}{2\sigma^2}\sum_{i = 1}^n \hat{\epsilon}_i^2\\
\end{aligned}
$$


## ML Estimators - $\sigma^2$

$$
\frac{\partial}{\partial \sigma^2} \left[\frac{n}{2} \log(2\pi\sigma^2) + \frac{1}{2\sigma^2}\sum_{i = 1}^n \hat{\epsilon}_i^2\right]
= \cdots
= 0
$$

## ML Estimators - $\sigma^2$

$$
\begin{aligned}
\frac{\partial}{\partial \sigma^2} \left[\frac{n}{2} \log(2\pi\sigma^2) + \frac{1}{2\sigma^2}\sum_{i = 1}^n \hat{\epsilon}_i^2\right]
&= \frac{n}{2} \frac{1}{\sigma^2} - \frac{1}{2\sigma^4}\sum_{i = 1}^n \hat{\epsilon}_i^2 = 0
\end{aligned}
$$
And we get:

$$
\hat{\sigma}^2_{ML} = \frac{1}{n}\sum_{i = 1}^n \hat{\epsilon}_i^2
$$

## ML Estimators - $\sigma^2$ - Remarks

* The ML estimator **is different** from the unbiased estimator of the variance we saw earlier.

$$
\hat{\sigma}^2_{ML} = \frac{1}{n}\sum_{i = 1}^n \hat{\epsilon}_i^2
\neq
\frac{1}{n-2}\sum_{i = 1}^n \hat{\epsilon}_i^2 = \hat{\sigma}^2
$$

>* The ML estimator of the variance is **biased**:
$$
\mathbb{E}[\hat{\sigma}^2_{ML}] = \frac{n - 2}{n}\sigma^2 \neq \sigma^2
$$

<!-- ************************************************************************ -->
# Distribution of the Coefficients - $\sigma^2$ known
<!-- ************************************************************************ -->

## Moments of the estimators

* We already know the moments of the estimators:

$$
\begin{aligned}
\mathbb{E}\left[
\begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{pmatrix}
\right]
&=
\begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix}
\\
\mathbb{Var}\left[
\begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{pmatrix}
\right]
&=
\frac{\sigma^2}{n}
\begin{pmatrix}
1 + \frac{\bar{x}^2}{s_{\mathbf{x}}^2} & -\frac{\bar{x}}{s_{\mathbf{x}}^2}\\
-\frac{\bar{x}}{s_{\mathbf{x}}^2} & \frac{1}{s_{\mathbf{x}}^2}
\end{pmatrix}
= \sigma^2 \mathbf{V}_n
\end{aligned}
$$

>* What can we say about their distribution ?

## Distribution of $(\hat{\beta}_0, \hat{\beta}_1)$

When the variance $\sigma^2$ is known, we find that the vector of estimators
$(\hat{\beta}_0, \hat{\beta}_1)$ is a **Gaussian vector**,
with expectation $(\beta_0, \beta_1)$ and variance $\sigma^2 \mathbf{V}_n$:

$$
\begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{pmatrix}
\sim
\mathcal{N}\left(
\begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix};
\sigma^2 \mathbf{V}_n
\right)
$$

>* Proof delayed to next chapter (multivariate regression).

## Simulated dataset

Simulate according to the model :
$$\mathbf{y} = -2 \cdot \mathbb{1} + 3 \cdot \mathbf{x} + \boldsymbol{\epsilon}$$


```{r sim1, echo = FALSE}
## Set the seed (Quia Sapientia)
set.seed(12890926)
## Number of samples
n <- 100
## vector of x
x_test <- runif(n, -2, 2)
## coefficients
beta_0 <- -2; beta_1 <- 3
## epsilon
error_test <- rnorm(n, mean = 0, sd = 10)
## y = 2 + 3 * x + epsilon
y_test <- beta_0 + beta_1 * x_test + error_test
```

```{r sim2, echo=FALSE}
ols_coef <- coef(lm(y_test ~ x_test))
beta_hat_0 <- ols_coef[1]
beta_hat_1 <- ols_coef[2]
```

```{r sim4, echo=FALSE}
## Number of replicates
r <- 1000
## Error replicates (matrix n x r)
error_test_all <- matrix(rnorm(n * r, mean = 0, sd = 10), nrow = n)
## Simulations replicates
y_test_all <- beta_0 + beta_1 * x_test + error_test_all
## Regression replicates
coefs_all <- apply(y_test_all, 2, function(y) coef(lm(y ~ x_test)))
```

```{r sim5, echo=FALSE, fig.height=3, fig.width=7.4}
ols_coef <- coef(lm(y_test ~ x_test))
beta_hat_0 <- ols_coef[1]
beta_hat_1 <- ols_coef[2]

ccc <- hcl.colors(5)

par(mfrow = c(1, 2), mar = c(5, 4, 1, 2))
### First plot
## Dataset
plot(x_test, y_test, pch = 16, cex = 0.7, ylab = "y_test")
## Regression line
abline(a = beta_hat_0, b = beta_hat_1, col = "deepskyblue2", lwd = 2)
## Ideal line
abline(a = beta_0, b = beta_1, col = "black", lwd = 2)

### Second plot
xxp <- -600:600/100
plot.new()
plot.window(xlim = c(-4, 6), ylim = c(0, 10))
axis(1, at = -4:6)
# beta hat 0
points(beta_hat_0, 5, pch = 16, col = ccc[1])
segments(beta_hat_0, 5, beta_hat_0, -1, lty = 2, col = ccc[1])
text(beta_hat_0, 0.5, label = expression(hat(beta)[0]), pos = 2, col = ccc[1])
# beta hat 1
points(beta_hat_1, 5, pch = 16, col = ccc[3])
segments(beta_hat_1, 5, beta_hat_1, -1, lty = 2, col = ccc[3])
text(beta_hat_1, 0.5, label = expression(hat(beta)[1]), pos = 2, col = ccc[3])
```


## Simulated dataset : replicates

Simulate according to the model :
$$\mathbf{y} = -2 \cdot \mathbb{1} + 3 \cdot \mathbf{x} + \boldsymbol{\epsilon}$$

```{r sim6, echo=FALSE, fig.height=3, fig.width=7.4}
par(mfrow = c(1, 2), mar = c(5, 4, 1, 2))

### Second plot
i <- 5
## Dataset
plot(x_test, y_test_all[, i], pch = 16, cex = 0.7, ylab = "y_test_3")
## Regression lines
abline(a = coefs_all[1, i], b = coefs_all[2, i], col = "deepskyblue2", lwd = 2)
## Ideal line
abline(a = beta_0, b = beta_1, col = "black", lwd = 2)

### Second plot
xxp <- -600:600/100
plot.new()
plot.window(xlim = c(-4, 6), ylim = c(0, 10))
axis(1, at = -4:6)
# beta hat 0
points(beta_hat_0, 5, pch = 16, col = ccc[1])
segments(beta_hat_0, 5, beta_hat_0, -1, lty = 2, col = ccc[1])
points(coefs_all[1, i], 5, pch = 16, col = ccc[1])
segments(coefs_all[1, i], 5, coefs_all[1, i], -1, lty = 2, col = ccc[1])
text(coefs_all[1, i], 0.5, label = expression(hat(beta)[0]), pos = 2, col = ccc[1])
# beta hat 1
points(beta_hat_1, 5, pch = 16, col = ccc[3])
segments(beta_hat_1, 5, beta_hat_1, -1, lty = 2, col = ccc[3])
points(coefs_all[2, i], 5, pch = 16, col = ccc[3])
segments(coefs_all[2, i], 5, coefs_all[2, i], -1, lty = 2, col = ccc[3])
text(coefs_all[2, i], 0.5, label = expression(hat(beta)[1]), pos = 2, col = ccc[3])
```

## Simulated dataset : replicates

Simulate according to the model :
$$\mathbf{y} = -2 \cdot \mathbb{1} + 3 \cdot \mathbf{x} + \boldsymbol{\epsilon}$$

```{r sim7, echo=FALSE, fig.height=3, fig.width=7.4}
par(mfrow = c(1, 2), mar = c(5, 4, 1, 2))

### Second plot
i <- 10
## Dataset
plot(x_test, y_test_all[, i], pch = 16, cex = 0.7, ylab = "y_test_3")
## Regression lines
abline(a = coefs_all[1, i], b = coefs_all[2, i], col = "deepskyblue2", lwd = 2)
## Ideal line
abline(a = beta_0, b = beta_1, col = "black", lwd = 2)

### Second plot
xxp <- -600:600/100
plot.new()
plot.window(xlim = c(-4, 6), ylim = c(0, 10))
axis(1, at = -4:6)
# beta hat 0
points(beta_hat_0, 5, pch = 16, col = ccc[1])
segments(beta_hat_0, 5, beta_hat_0, -1, lty = 2, col = ccc[1])
points(coefs_all[1, i], 5, pch = 16, col = ccc[1])
segments(coefs_all[1, i], 5, coefs_all[1, i], -1, lty = 2, col = ccc[1])
points(coefs_all[1, 5], 5, pch = 16, col = ccc[1])
segments(coefs_all[1, 5], 5, coefs_all[1, 5], -1, lty = 2, col = ccc[1])
text(coefs_all[1, 5], 0.5, label = expression(hat(beta)[0]), pos = 2, col = ccc[1])
# beta hat 1
points(beta_hat_1, 5, pch = 16, col = ccc[3])
segments(beta_hat_1, 5, beta_hat_1, -1, lty = 2, col = ccc[3])
points(coefs_all[2, 5], 5, pch = 16, col = ccc[3])
segments(coefs_all[2, 5], 5, coefs_all[2, 5], -1, lty = 2, col = ccc[3])
points(coefs_all[2, i], 5, pch = 16, col = ccc[3])
segments(coefs_all[2, i], 5, coefs_all[2, i], -1, lty = 2, col = ccc[3])
text(coefs_all[2, i], 0.5, label = expression(hat(beta)[1]), pos = 2, col = ccc[3])
```

## Simulated dataset : replicates

Simulate according to the model :
$$\mathbf{y} = -2 \cdot \mathbb{1} + 3 \cdot \mathbf{x} + \boldsymbol{\epsilon}$$

```{r sim8, echo=FALSE, fig.height=3, fig.width=7.4}
par(mfrow = c(1, 2), mar = c(5, 4, 1, 2))

### Second plot
## Dataset
plot(x_test, y_test, pch = 16, cex = 0)
## all other regression lines
sum_beta_0_hat <- 0
sum_beta_1_hat <- 0
for (i in 1:r) {
  abline(a = coefs_all[1, i], b = coefs_all[2, i], col = "deepskyblue2", lwd = 0.5)
}
## Ideal line
abline(a = beta_0, b = beta_1, col = "black", lwd = 2)

### Hist
xxp <- -600:600/100
plot(xxp, dnorm(xxp, beta_1, 1 / sd(x_test)), lwd = 2, type = "l", col = ccc[3],
     xlim = c(-4, 6),
     main = "Histogram of Coefficients", xlab = "Coefficients", ylab = "Density")
lines(xxp, dnorm(xxp, beta_0, 1), lwd = 2, col = ccc[1])
bb <- hist(coefs_all[2, ], plot = FALSE)
plot(bb, add = TRUE, col = ccc[3], density = 15, freq = FALSE)
aa <- hist(coefs_all[1, ], plot = FALSE)
plot(aa, add = TRUE, col = ccc[1], density = 15, freq = FALSE)
# legend("topleft",
#        c(expression(paste(mu == beta[0], "   ; ", sigma^2 == sigma^2/n %*% (1+bar(x)^2/s[x]^2))),
#          expression(paste(mu == beta[1], "   ; ", sigma^2 == sigma^2/n %*%  1 / s[x]^2))),
#        col = c("black", ccc[3]),
#        lwd = 1, lty = 1)
```

$$
\hat{\beta}_0 \sim \mathcal{N}\left(\beta_0, \frac{\sigma^2}{n}\left(1 + \frac{\bar{x}^2}{s_{\mathbf{x}}^2}\right)\right)
\quad
\hat{\beta}_1 \sim \mathcal{N}\left(\beta_1, \frac{\sigma^2}{n}\frac{1}{s_{\mathbf{x}}^2}\right)
$$

<!-- ************************************************************************ -->
# Distribution of the Variance - $\sigma^2$ known
<!-- ************************************************************************ -->

## Unbiased estimator of the Variance

$$
\hat{\sigma}^2 = \frac{1}{n-2}\sum_{i = 1}^n \hat{\epsilon}_i^2
\qquad
\mathbb{E}[\hat{\sigma}^2] = \sigma^2
$$

>* **Question**: Distribution of $\hat{\sigma}^2$ ?

## Distribution of $\hat{\sigma}^2$

$$
\frac{n-2}{\sigma^2}\hat{\sigma}^2 \sim \chi^2_{n-2}
$$

$~$

>* The variance estimator $\hat{\sigma}^2$ is distributed according to a
**chi squared** distribution with **n-2** degrees of freedom.

$~$

>* $\hat{\beta}$ and $\hat{\sigma}^2$ are **independent**.

$~$

>* Proof delayed to next chapter (multivariate regression).

## Reminder: Chi Squared Distribution 1/3

Let $X_1, \dotsc, X_p$ be $p$ standard normal iid rv : $X_i \sim \mathcal{N}(0, 1)$.
Then 
$$
X = \sum_{i = 1}^p X_i^2 \sim \chi^2_p
$$ 

is a Chi squared r.v. with $p$ degrees of freedom.

We have:
$$
\mathbb{E}[X] = p
\quad
\mathbb{Var}[X] = 2p.
$$

It converges in distribution towards the Gaussian with matching moments.

## Reminder: Chi Squared Distribution 2/3

```{r chisq, echo=FALSE, fig.height=5.5, fig.width=8}
x <- 0:800 / 100
ccc <- hcl.colors(6)
# 1
p <- 1
plot(x, dchisq(x, p), type = "l", ylab = "f(x)", ylim = c(0, 0.5), col = ccc[1], lwd = 2)
segments(x0 = p, y0 = -1, x1 = p, y1 = dchisq(p, p), lty = 2, col = ccc[1])
# 2 - 5
for (p in 2:5) {
  lines(x, dchisq(x, p), col = ccc[p], lwd = 2)
  segments(x0 = p, y0 = -1, x1 = p, y1 = dchisq(p, p), lty = 2, col = ccc[p])
}
# legend
legend("topright",
       c(expression(p == 1), expression(p == 2), expression(p == 3), expression(p == 4), expression(p == 5)),
       col = ccc,
       lwd = 2, lty = 1)
```

## Reminder: Chi Squared Distribution 3/3

```{r chisq2, echo=FALSE, fig.height=5.5, fig.width=8}
x <- 0:10000 / 100
# 50
p <- 50
plot(x, dchisq(x, p), type = "l", ylab = "f(x)", ylim = c(0, 0.05), col = ccc[1], lwd = 2)
segments(x0 = p, y0 = -1, x1 = p, y1 = dchisq(p, p), lty = 2, col = ccc[1])
# normal
mu <- p; sigma2 <- 2*p
lines(x, dnorm(x, mu, sqrt(sigma2)), col = ccc[4])
# legend
legend("topright",
       c(expression(p == 50),
         expression(paste(mu == 50, "   ; ", sigma^2 == 100))),
       col = ccc[c(1, 4)],
       lwd = c(2, 1), lty = 1)
```


<!-- ************************************************************************ -->
# Distribution of the Coefficients - $\sigma^2$ unknown
<!-- ************************************************************************ -->

## Distribution of $(\hat{\beta}_0, \hat{\beta}_1)$

When $\sigma^2$ is known:

$$
\begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{pmatrix}
\sim
\mathcal{N}\left(
\begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix};
\sigma^2 \mathbf{V}_n
\right)
\qquad
\mathbf{V}_n= \frac{1}{n}
\begin{pmatrix}
1 + \frac{\bar{x}^2}{s_{\mathbf{x}}^2} & -\frac{\bar{x}}{s_{\mathbf{x}}^2}\\
-\frac{\bar{x}}{s_{\mathbf{x}}^2} & \frac{1}{s_{\mathbf{x}}^2}
\end{pmatrix}
$$

i.e.

$$
\frac{\hat{\beta}_0 - \beta_0}{\sqrt{\sigma^2(1 + \bar{x}^2/s_{\mathbf{x}}^2)/n}}
\sim
\mathcal{N}(0, 1)
\qquad
\frac{\hat{\beta}_1 - \beta_1}{\sqrt{\sigma^2(1/s_{\mathbf{x}}^2)/n}}
\sim
\mathcal{N}(0, 1)
$$

>* **Problem** $\sigma^2$ is generally *unknown*.

>* **Solution** Replace $\sigma^2$ by $\hat{\sigma}^2$.

>* How is the distribution changed ?

## Distribution of $(\hat{\beta}_0, \hat{\beta}_1)$

When $\sigma^2$ is known:

$$
\frac{\hat{\beta}_0 - \beta_0}{\sqrt{\sigma^2(1 + \bar{x}^2/s_{\mathbf{x}}^2)/n}}
\sim
\mathcal{N}(0, 1)
\qquad
\frac{\hat{\beta}_1 - \beta_1}{\sqrt{\sigma^2(1/s_{\mathbf{x}}^2)/n}}
\sim
\mathcal{N}(0, 1)
$$

When $\sigma^2$ is **unknown**:
$$
\frac{\hat{\beta}_0 - \beta_0}{\sqrt{\hat{\sigma}^2(1 + \bar{x}^2/s_{\mathbf{x}}^2)/n}}
\sim
\mathcal{T}_{n-2}
\qquad
\frac{\hat{\beta}_1 - \beta_1}{\sqrt{\hat{\sigma}^2(1/s_{\mathbf{x}}^2)/n}}
\sim
\mathcal{T}_{n-2}
$$

* Replace standard Gaussian by **Student** distribution.

## Reminder: Student Distribution 1/2

Let $Z$ be a standard normal rv : $Z \sim \mathcal{N}(0, 1)$,  
and $X$ a chi squared rv with $p$ degrees of freedom: $X \sim \chi^2_p$,
$Z$ and $X$ independent.
Then 
$$
T = \frac{Z}{\sqrt{X/p}} \sim \mathcal{T}_p
$$ 

is a Student r.v. with $p$ degrees of freedom.

We have (for $p > 2$):
$$
\mathbb{E}[T] = 0
\quad
\mathbb{Var}[T] = \frac{p}{p-2}.
$$

It converges in distribution towards the standard Gaussian.

## Reminder: Student Distribution 2/2

```{r student, echo=FALSE, fig.height=5.5, fig.width=8}
x <- -600:600 / 100
ccc <- hcl.colors(6)
# 1
p <- 1
plot(x, dt(x, p), type = "l", ylab = "f(x)", ylim = c(0, 0.4), col = ccc[1], lwd = 2)
segments(x0 = 0, y0 = -1, x1 = 0, y1 = dt(0, p), lty = 2, col = ccc[1])
# 2 - 5
k <- 1
for (p in c(3, 10)) {
  k <- k + 1
  lines(x, dt(x, p), col = ccc[k], lwd = 2)
}
# normal
mu <- 0; sigma2 <- 1
lines(x, dnorm(x, mu, sqrt(sigma2)), col = ccc[5], lwd = 2)
# legend
legend("topright",
       c(expression(p == 1), expression(p == 3), expression(p == 10),
         expression(paste(mu == 0, "   ; ", sigma^2 == 1))),
       col = ccc[c(1:k, 5)],
       lwd = 2, lty = 1)
```

<!-- ************************************************************************ -->
# Confidence Intervals
<!-- ************************************************************************ -->

## Confidence intervals

$$
\frac{\hat{\beta}_0 - \beta_0}{\sqrt{\hat{\sigma}^2_0}}
\sim
\mathcal{T}_{n-2}
\qquad
\frac{\hat{\beta}_1 - \beta_1}{\sqrt{\hat{\sigma}^2_1}}
\sim
\mathcal{T}_{n-2}\\
\frac{n-2}{\sigma^2}\hat{\sigma}^2 \sim \chi^2_{n-2}
$$

>* With probability $1-\alpha$:
$$
\begin{aligned}
\beta_0 &\in \left[\hat{\beta}_0 \pm t_{n-2}(1 - \alpha/2) \sqrt{\hat{\sigma}_0^2}\right]\\
\beta_1 &\in \left[\hat{\beta}_1 \pm t_{n-2}(1 - \alpha/2) \sqrt{\hat{\sigma}_1^2}\right]\\
\sigma^2 &\in \left[\frac{(n-2)\hat{\sigma}^2}{c_{n-2}(1 - \alpha/2)}; \frac{(n-2)\hat{\sigma}^2}{c_{n-2}(\alpha/2)}\right]
\end{aligned}
$$

## Simulated dataset

$$\mathbf{y} = -2 \cdot \mathbb{1} + 3 \cdot \mathbf{x} + \boldsymbol{\epsilon}$$


```{r simci, echo = FALSE}
## Set the seed (Quia Sapientia)
set.seed(12890926)
## Number of samples
n <- 100
## vector of x
x_test <- runif(n, -2, 2)
## coefficients
beta_0 <- -2; beta_1 <- 3
## epsilon
error_test <- rnorm(n, mean = 0, sd = 1)
## y = 2 + 3 * x + epsilon
y_test <- beta_0 + beta_1 * x_test + error_test
```

```{r simciplot, echo=FALSE, fig.height=3, fig.width=8}
ols_coef <- coef(lm(y_test ~ x_test))
beta_hat_0 <- ols_coef[1]
beta_hat_1 <- ols_coef[2]

ccc <- hcl.colors(5)

par(mfrow = c(1, 2), mar = c(5, 4, 1, 2))
### First plot
## Dataset
plot(x_test, y_test, pch = 16, cex = 0.7, ylab = "y_test")
## Regression line
abline(a = beta_hat_0, b = beta_hat_1, col = "deepskyblue2", lwd = 2)
## Ideal line
# abline(a = beta_0, b = beta_1, col = "black", lwd = 2)

### Student Distribution
x <- -5500:5500 / 1000
# 1
p <- n - 2
plot(x, dt(x, p), type = "l", ylab = "density", ylim = c(0, 0.4), col = ccc[1], lwd = 2, xlab = "")
abline(h = 0)
# quantiles for 1 - alp = 95% interval
alp <- 0.05
quants <- qt(c(alp / 2, 1 - alp / 2), p)
segments(x0 = quants, y0 = -1, x1 = quants, y1 = dt(0, p), lty = 2, col = ccc[3])
axis(side = 1, at = quants, labels = c(expression(t[n-2]^(alpha/2)), expression(t[n-2]^(1 - alpha/2))), col.ticks = ccc[3], col.axis = ccc[3], las = 1, padj = 1.2)
# # beta hat 1
# segments(beta_hat_1, 5, beta_hat_1, -1, lty = 2, col = ccc[3])
# axis(side = 1, at = beta_hat_1, labels = expression(hat(beta)[1]), col.ticks = ccc[3], col.axis = ccc[3], las = 1, padj = 1.2)
# Color area under the curve
x_ci <- (quants[1]*1000):(quants[2]*1000)/1000
polygon(c(x_ci, rev(x_ci)),
        c(dt(x_ci, p), rep(0, length(x_ci))),
        col = ccc[5], border = NA, density = 20)
# legend
legend("topright",
       c(expression(T[n-2])),
       col = ccc[1],
       lwd = 2, lty = 1)
```

>* With probability $1-\alpha$:
$$
\begin{aligned}
t_{n-2}(\alpha/2) \leq \frac{\hat{\beta}_1 - \beta_1}{\sqrt{\hat{\sigma}^2_1}} \leq t_{n-2}(1 - \alpha/2)
\end{aligned}
$$

## Simulated dataset - `R` output {.smaller}

$$\mathbf{y} = -2 \cdot \mathbb{1} + 3 \cdot \mathbf{x} + \boldsymbol{\epsilon}$$

```{r simcir, echo = TRUE}
linreg <- lm(y_test ~ x_test)
summary(linreg)
```

<!-- ************************************************************************ -->
# Prediction Intervals
<!-- ************************************************************************ -->

## Prediction error

Recall that the prediction error $\hat{\epsilon}_{n+1} = y_{n+1} - \hat{y}_{n+1}$
is such that:
$$
\begin{aligned}
\mathbb{E}[\hat{\epsilon}_{n+1}] &= 0\\
\mathbb{Var}[\hat{\epsilon}_{n+1}] &= \sigma^2 \left(1 + \frac{1}{n} + \frac{1}{ns_{\mathbf{x}}^2} (x_{n+1} - \bar{x})^2\right)\\
\end{aligned}
$$

## Prediction Distribution

We get (variance known):

$$
y_{n+1} - \hat{y}_{n+1}
\sim
\mathcal{N}\left(0, \sigma^2 \left(1 + \frac{1}{n} + \frac{1}{ns_{\mathbf{x}}^2} (x_{n+1} - \bar{x})^2\right)\right)
$$

and (variance unknown):
$$
\frac{y_{n+1} - \hat{y}_{n+1}}{\sqrt{\hat{\sigma}^2 \left(1 + \frac{1}{n} + \frac{1}{ns_{\mathbf{x}}^2} (x_{n+1} - \bar{x})^2\right)}}
\sim
\mathcal{T}_{n-2}
$$

<!-- ************************************************************************ -->
# Advertising Dataset
<!-- ************************************************************************ -->

## Advertising

```{r ad1, message=FALSE}
library(here)
ad <- read.csv(here("data", "Advertising.csv"), row.names = "X")
head(ad)
```

* `TV`, `radio`, `newspaper`: advertising budgets (thousands of $)
* `sales`: number of sales (thousands of units)

## Advertising - TV - 1/6

```{r ad2}
fit_tv <- lm(sales ~ TV, data = ad)
plot(ad$TV, ad$sales)
abline(fit_tv, col = "blue")
```

## Advertising - TV - 2/6 {.smaller}

```{r ad3}
summary(fit_tv)
```

## Advertising - TV - 3/6

>- Is there a relationship between TV advertising budget and sale ?
>    - Student t test: p values < 2e-16
>- How strong is the relationship ?
>    - RSS = 3.259, RSS / $\bar{y}$ = 23% (rough percentage of error)
>    - $R^2$: 0.6119 : Only around 61\% of the variance explained.
>- How large is the effect of TV on sales ?
>    - $\hat{\beta}_1 \approx 0.047537 \pm 1.96 \times 0.002691$
>    - CI away from 0
>    - When I spent 1000\$ more in TV, I sell about 47.5 more units.

## Advertising - TV - 4/6

```{r ad4, eval = FALSE}
## lm fit
fit_tv <- lm(sales ~ TV, data = ad)
plot(ad$TV, ad$sales)
abline(fit_tv, col = "black")
## colors
contrast_col <- hcl.colors(2)
## Confidence interval
newx <- seq(0, 300, by = 1)
conf_int <- predict(fit_tv, data.frame(TV = newx), interval = "conf")
# Plot
lines(newx, conf_int[, 2], col = contrast_col[1], lty = 2, lwd = 2)
lines(newx, conf_int[, 3], col = contrast_col[1], lty = 2, lwd = 2)
# Prediction intervals
pred_int <- predict(fit_tv, data.frame(TV = newx), interval = "pred")
# Plot
lines(newx, pred_int[, 2], col = contrast_col[2], lty = 2, lwd = 2)
lines(newx, pred_int[, 3], col = contrast_col[2], lty = 2, lwd = 2)
```

## Advertising - TV - 5/6

* 95\% confidence interval (dark purple) and prediction interval (light yellow)

```{r ad5, echo = FALSE}
## lm fit
fit_tv <- lm(sales ~ TV, data = ad)
plot(ad$TV, ad$sales)
abline(fit_tv, col = "black")
## colors
contrast_col <- hcl.colors(2)
## Confidence interval
newx <- seq(0, 300, by = 1)
conf_int <- predict(fit_tv, data.frame(TV = newx), interval = "conf")
# Plot
lines(newx, conf_int[, 2], col = contrast_col[1], lty = 2, lwd = 2)
lines(newx, conf_int[, 3], col = contrast_col[1], lty = 2, lwd = 2)
# Prediction intervals
pred_int <- predict(fit_tv, data.frame(TV = newx), interval = "pred")
# Plot
lines(newx, pred_int[, 2], col = contrast_col[2], lty = 2, lwd = 2)
lines(newx, pred_int[, 3], col = contrast_col[2], lty = 2, lwd = 2)
```

## Advertising - TV - 6/6

>- How accurately can we predict future sales ?
>   - See prediction intervals

>- Is the relationship linear ?
>   - Does it look like it ? More on that later.
>- Which media contribute to sales ?
>   - Start again with newspapers and radio.
>- Is there synergy among the media ?
>   - ??

>- To answer these questions better, we need **multiple** linear regression.
>   - Next chapter.

<!-- ************************************************************************ -->
# Joint Distribution of the Coefficients - $\sigma^2$ unknown
<!-- ************************************************************************ -->

## Distribution of $(\hat{\beta}_0, \hat{\beta}_1)$

When $\sigma^2$ is known:

$$
\hat{\boldsymbol{\beta}} = 
\begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{pmatrix}
\sim
\mathcal{N}\left(
\boldsymbol{\beta} = 
\begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix};
\sigma^2 \mathbf{V}_n
\right)
$$

<!-- i.e. -->

<!-- $$ -->
<!-- \frac{1}{\sqrt{\sigma^2}}\mathbf{V}_n^{-1/2}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right) -->
<!-- \sim -->
<!-- \mathcal{N}\left( -->
<!-- \mathbf{0}; -->
<!-- \mathbf{I}_n -->
<!-- \right) -->
<!-- $$ -->

<!-- where $\mathbf{V}_n^{-1/2}$ is a matrix such that  -->
<!-- $\mathbf{V}_n^{-1/2} \mathbf{V}_n^{-1} [\mathbf{V}_n^{-1/2}]^T = \mathbf{I}_n$ -->
<!-- (Cholesky decomposition) -->

<!-- ## Reminder: <br> Linear function of a Gaussian -->

<!-- If $X \sim \mathcal{N}(\mu, \sigma^2)$, then for any $(a, b) \in \mathbb{R}^2$,   -->
<!-- $$ -->
<!-- Y = aX + b \sim \mathcal{N}(a\mu + b, a^2\sigma^2). -->
<!-- $$ -->

<!-- If $\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{\Sigma})$, -->
<!-- then for any $\mathbf{a}$ matrix and $\mathbf{b}$ vector,   -->
<!-- $$ -->
<!-- \mathbf{Y} = \mathbf{a}\mathbf{X} + \mathbf{b}  -->
<!-- \sim \mathcal{N}(\mathbf{a}\boldsymbol{\mu} + \mathbf{b}, \mathbf{a}\mathbf{\Sigma}\mathbf{a}^T). -->
<!-- $$ -->

<!-- * **Proof**: Exercise.   -->
<!-- [Hint: use the characteristic function.] -->


<!-- ## Distribution of $(\hat{\beta}_0, \hat{\beta}_1)$ -->

<!-- When $\sigma^2$ is known: -->

<!-- $$ -->
<!-- \frac{1}{\sqrt{\sigma^2}}\mathbf{V}_n^{-1/2}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right) -->
<!-- \sim -->
<!-- \mathcal{N}\left( -->
<!-- \mathbf{0}; -->
<!-- \mathbf{I}_n -->
<!-- \right) -->
<!-- $$ -->

When $\sigma^2$ is **unknown**:
$$
\frac{1}{2\hat{\sigma}^2}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)^T \mathbf{V}_n^{-1}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)
\sim
\mathcal{F}^2_{n-2}
$$

* with $\mathcal{F}^2_{n-2}$ the **Fisher** distribution.

## Reminder: Fisher Distribution 1/2

Let $U_1 \sim \chi^2_{p_1}$ and $U_2 \sim \chi^2_{p_2}$,
$U_1$ and $U_2$ independent.
Then 
$$
F = \frac{U_1/p_1}{U_2/p_2} \sim \mathcal{F}^{p_1}_{p_2}
$$ 

is a Fisher r.v. with $(p_1, p_2)$ degrees of freedom.

We have (for $p_2 > 2$):
$$
\mathbb{E}[F] =  \frac{p_2}{p_2 - 2}.
$$

When $p_2$ goes to infinity, $\mathcal{F}^{p_1}_{p_2}$ converges in distribution
to $\frac{1}{p_1}\chi^2_{p_1}$.

## Reminder: Fisher Distribution 2/2

```{r fisher, echo=FALSE, fig.height=5.5, fig.width=8}
x <- 0:600 / 100
ccc <- hcl.colors(6)
# 1
p1 <- 2
p <- 3
plot(x, df(x, p1, p), type = "l", ylab = "f(x)", ylim = c(0, 1), col = ccc[1], lwd = 2)
segments(x0 = p/(p-2), y0 = -1, x1 = p/(p-2), y1 = df(p/(p-2), p1, p), lty = 2, col = ccc[1])
# 2 - 5
k <- 1
for (p in c(5, 10)) {
  k <- k + 1
  lines(x, df(x, p1, p), col = ccc[k], lwd = 2)
  segments(x0 = p/(p-2), y0 = -1, x1 = p/(p-2), y1 = df(p/(p-2), p1, p), lty = 2, col = ccc[k])
}
# normal
lines(x, 2 * dchisq(2 * x, p1), col = ccc[5], lwd = 2)
# legend
legend("topright",
       c(expression(paste(p[1] == 2, "   ; ", p[2] == 3)),
         expression(paste(p[1] == 2, "   ; ", p[2] == 5)),
         expression(paste(p[1] == 2, "   ; ", p[2] == 10)),
         expression(paste(p == 2, " [", chi[p]^2 / p, "]"))),
       col = ccc[c(1:k, 5)],
       lwd = 2, lty = 1)
```

<!-- ************************************************************************ -->
# Confidence Region
<!-- ************************************************************************ -->

## Confidence Intervals

* Marginal distributions:

$$
\frac{\hat{\beta}_0 - \beta_0}{\sqrt{\hat{\sigma}^2_0}}
\sim
\mathcal{T}_{n-2}
\qquad
\frac{\hat{\beta}_1 - \beta_1}{\sqrt{\hat{\sigma}^2_1}}
\sim
\mathcal{T}_{n-2}
$$

* Give confidence intervals:

$$
\begin{aligned}
\beta_0 &\in \left[\hat{\beta}_0 \pm t_{n-2}(1 - \alpha/2) \sqrt{\hat{\sigma}_0^2}\right]\\
\beta_1 &\in \left[\hat{\beta}_1 \pm t_{n-2}(1 - \alpha/2) \sqrt{\hat{\sigma}_1^2}\right]\\
\end{aligned}
$$

## Confidence Region

* Joint distribution:

$$
\frac{1}{2\hat{\sigma}^2}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)^T \mathbf{V}_n^{-1}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)
\sim
\mathcal{F}^2_{n-2}
$$

* Gives confidence region:

$$
\frac{1}{2\hat{\sigma}^2}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)^T \mathbf{V}_n^{-1}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)
\leq
f_{n-2}^2(1 - \alpha)
$$

Since $\mathbf{V}_n^{-1}$ is positive definite, this is the equation of an elliptic region.

## Simulated dataset

$$\mathbf{y} = -2 \cdot \mathbb{1} + 3 \cdot \mathbf{x} + \boldsymbol{\epsilon}$$


```{r simcr, echo = FALSE}
## Set the seed (Quia Sapientia)
set.seed(12890926)
## Number of samples
n <- 100
## vector of x
x_test <- runif(n, -2, 2)
## coefficients
beta_0 <- -2; beta_1 <- 3
## epsilon
error_test <- rnorm(n, mean = 0, sd = 1)
## y = 2 + 3 * x + epsilon
y_test <- beta_0 + beta_1 * x_test + error_test
```

```{r simcrplot, echo=FALSE, message=FALSE, fig.height=3, fig.width=8}
## lm fit
fit_lm <- lm(y_test ~ x_test)
## Plot
par(mfrow = c(1, 2), mar = c(5, 5, 1, 1))
### Data and line plot
## Dataset
plot(x_test, y_test, pch = 16, cex = 0.7, ylab = "y_test")
## Regression line
abline(fit_lm, col = "deepskyblue2", lwd = 2)

### Confidence region
ccc <- hcl.colors(5)
library(ellipse)
cr <- ellipse(fit_lm, level = 0.95)
plot(cr, type = 'l',
     xlab = expression(hat(beta)[0]),
     ylab = expression(hat(beta)[1]),
     col = ccc[1], lwd = 2)
points(fit_lm$coefficients[1], fit_lm$coefficients[2])

### Confidence intervals
ci <- confint(fit_lm, level = 0.95)
lines(ci[1,], rep(fit_lm$coefficients[2], 2), col = ccc[4], lwd = 2)
lines(rep(fit_lm$coefficients[1], 2), ci[2,], col = ccc[4], lwd = 2)
polygon(ci[1, c(1, 2, 2, 1)], ci[2, c(1, 1, 2, 2)], border = ccc[4])
```

Difference between:  

* **confidence intervals** (green) and the
* **confidence region** (purple).

